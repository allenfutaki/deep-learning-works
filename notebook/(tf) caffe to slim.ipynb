{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"/home/allen/Documents/caffe/python\")\n",
    "import caffe\n",
    "import numpy as np\n",
    "\n",
    "import caffe.proto.caffe_pb2 as caffepb\n",
    "from google.protobuf import text_format\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deploy = \"/media/allen/mass/caffe-tensorflow/mobilenet_multitask_20190322.temp.cp.prototxt\"\n",
    "caffemodel = \"/media/allen/mass/caffe-tensorflow/mobilenet_multitask_20190322.temp.caffemodel\"\n",
    "# deploy = \"/media/allen/mass/caffe-tensorflow/20190220_fmobilenet_fc_align.temp.prototxt\"\n",
    "# caffemodel = \"/media/allen/mass/caffe-tensorflow/20190220_fmobilenet_fc_align.temp.caffemodel\"\n",
    "# net = caffe.Net(deploy, caffemodel, caffe.TEST)\n",
    "\n",
    "# net_params = caffepb.NetParameter()\n",
    "# f = open(deploy, 'rb')\n",
    "# net_str = f.read()\n",
    "# text_format.Merge(net_str, net_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network():\n",
    "    def __init__(self, deploy, caffemodel):\n",
    "        \n",
    "        net_params = caffepb.NetParameter()\n",
    "        f = open(deploy, 'rb')\n",
    "        net_str = f.read()\n",
    "        text_format.Merge(net_str, net_params)\n",
    "\n",
    "        self.weights = caffe.Net(deploy, caffemodel, caffe.TEST).params\n",
    "        self.caffe_graph = net_params\n",
    "        \n",
    "        self.caffe_layer_list = {}\n",
    "        self.set_caffe_layer_list()\n",
    "        \n",
    "        self.sess = tf.InteractiveSession()\n",
    "        self.graph = self.sess.graph\n",
    "        self.end_points = {}\n",
    "        self.var_data_map = {}\n",
    "        \n",
    "    def show_graph(self):\n",
    "        for i in self.graph.as_graph_def().node:\n",
    "            print(\"{:45} {:20}\".format(i.name, i.op))\n",
    "        \n",
    "    def set_caffe_layer_list(self):\n",
    "        for layer in self.caffe_graph.layer:\n",
    "            self.caffe_layer_list[layer.name] = layer\n",
    "        \n",
    "    def get_layer(self, l_name, level='next', target=None):\n",
    "        next_layers = []\n",
    "        if level == 'next':\n",
    "            tops = self.caffe_layer_list[l_name].top\n",
    "            for layer in self.caffe_graph.layer:\n",
    "                for top_name in tops:\n",
    "                    if top_name in layer.bottom:\n",
    "                        next_layers.append(layer)\n",
    "        else:\n",
    "            bottoms = self.caffe_layer_list[l_name].bottom\n",
    "            for layer in self.caffe_graph.layer:\n",
    "                for bottom_name in bottoms:\n",
    "                    if bottom_name in layer.top:\n",
    "                        next_layers.append(layer)\n",
    "                    \n",
    "        if target is not None:\n",
    "            for layer in next_layers:\n",
    "                if layer.type == target:\n",
    "                    return layer\n",
    "        return next_layers\n",
    "    \n",
    "    def record_tf_variable(self, name, data):\n",
    "        self.var_data_map[name] = data\n",
    "#         return tf.constant(value=weight, dtype=tf.float32)        \n",
    "    \n",
    "    def add_end_points(self, l, output):\n",
    "        self.end_points[l.name] = output\n",
    "               \n",
    "        \n",
    "    def add(self, l):     \n",
    "        if l.type == 'Input':\n",
    "            n, c, h, w = l.input_param.shape[0].dim\n",
    "            shape = [n, h, w, c]\n",
    "            output = tf.placeholder(tf.float32, shape=shape, name=l.name)\n",
    "            self.add_end_points(l, output)\n",
    "            \n",
    "        elif l.type == 'BatchNorm':\n",
    "            scale_layer = self.get_layer(l.name, target='Scale')            \n",
    "            w1 = self.weights[l.name]\n",
    "            raw_mean = w1[0].data\n",
    "            raw_var = w1[1].data\n",
    "            w2 = self.weights[scale_layer.name]\n",
    "            raw_scale = w2[0].data\n",
    "            raw_offset = w2[1].data\n",
    "            \n",
    "            mean = self.record_tf_variable(l.name+'/moving_mean', raw_mean)\n",
    "            var = self.record_tf_variable(l.name+'/moving_variance', raw_var)\n",
    "            scale = self.record_tf_variable(l.name+'/gamma', raw_scale)\n",
    "            offset = self.record_tf_variable(l.name+'/beta', raw_offset)\n",
    "            \n",
    "            bottoms = self.get_layer(l.name, level='prev')  \n",
    "            assert len(bottoms) == 1\n",
    "            inputs = self.end_points[bottoms[-1].name]\n",
    "            input_shape = inputs.get_shape()\n",
    "            if len(input_shape) == 2:                \n",
    "                inputs = tf.reshape(inputs, shape=[input_shape[0], 1, 1, input_shape[1]], name=l.name+\"_expand\")          \n",
    "                output = slim.batch_norm(inputs=inputs, center=True, scale=True, is_training=True, activation_fn=None, fused=True, scope=l.name)\n",
    "                output = tf.reshape(output, shape=[input_shape[0], input_shape[1]], name=l.name+\"_reduce\")  \n",
    "            else:\n",
    "                output = slim.batch_norm(inputs=inputs, center=True, scale=True, is_training=True, activation_fn=None, fused=True, scope=l.name)\n",
    "            self.add_end_points(scale_layer, output)\n",
    "        \n",
    "        elif l.type == 'Convolution':     \n",
    "            \n",
    "            params = l.convolution_param\n",
    "            c_o = params.num_output\n",
    "            bias = params.bias_term\n",
    "            group = params.group\n",
    "            p_h = params.pad_h\n",
    "            p_w = params.pad_w\n",
    "            k_h = params.kernel_h\n",
    "            k_w = params.kernel_w\n",
    "            s_h = params.stride_h\n",
    "            s_w = params.stride_w\n",
    "            dilation = params.dilation\n",
    "            \n",
    "            if s_h == 0 or s_w == 0:\n",
    "                s_h = s_w = params.stride[0]\n",
    "            if k_h == 0 or k_w == 0:\n",
    "                k_h = k_w = params.kernel_size[0]\n",
    "            if len(params.pad) > 0 and params.pad[0] > 0:\n",
    "                p_h = p_w = params.pad[0]\n",
    "                \n",
    "            bottoms = self.get_layer(l.name, level='prev')\n",
    "            inputs = self.end_points[bottoms[-1].name]\n",
    "            input_shape = inputs.get_shape()\n",
    "            \n",
    "            assert len(input_shape) == 4\n",
    "            _, _, _, c_i = input_shape\n",
    "            \n",
    "            if p_h > 0:\n",
    "                inputs = tf.pad(inputs, [[0, 0], [p_h, p_h], [p_w, p_w], [0, 0]], name=\"{}_pad\".format(l.name))\n",
    "                pad_type = 'VALID'\n",
    "            else:\n",
    "                if s_h > 1:\n",
    "                    pad_type = 'SAME'\n",
    "                else:\n",
    "                    pad_type = 'VALID'\n",
    "                    \n",
    "            raw_w = self.weights[l.name][0].data\n",
    "            if group == 1:\n",
    "                raw_w = raw_w.transpose((2, 3, 1, 0))\n",
    "                kernel = self.record_tf_variable(l.name+'/weights', raw_w)\n",
    "                output = slim.convolution2d(inputs=inputs, num_outputs=c_o, kernel_size=k_h, stride=s_h, padding=pad_type, scope=l.name, activation_fn=None)\n",
    "                if bias:\n",
    "                    raw_b = self.weights[l.name][1].data                  \n",
    "                else:\n",
    "                    raw_b = raw_b = np.zeros(c_o)\n",
    "                \n",
    "            biases = self.record_tf_variable(l.name+'/biases', raw_b)\n",
    "            elif group == c_i and group == c_o:                \n",
    "                raw_w = raw_w.transpose((2, 3, 0, 1))\n",
    "                kernel = self.record_tf_variable(l.name+'/weights', raw_w)\n",
    "                output = tf.nn.depthwise_conv2d(inputs, filter=kernel, strides=[1,s_h, s_w,1], rate=[1,1], padding=pad_type, name=l.name)\n",
    "            else:\n",
    "                raw_w = raw_w.transpose((2, 3, 1, 0))\n",
    "                \n",
    "                # Split the input into groups and then convolve each of them independently\n",
    "                input_groups = tf.split(axis=3, num_or_size_splits=group, value=inputs)\n",
    "                convolve = lambda i : output = slim.convolution2d(inputs=i, num_outputs=c_o, kernel_size=k_h, stride=s_h, padding=pad_type, scope=l.name, activation_fn=None)\n",
    "                output_groups = []\n",
    "                for i, inputs in enumerate(input_groups)\n",
    "                    output_groups.append(slim.convolution2d(inputs=inputs, num_outputs=c_o, kernel_size=k_h, stride=s_h, padding=pad_type, scope=l.name+\"_i\", activation_fn=None))\n",
    "                    kernel = self.record_tf_variable('{}_{}/weights'.format(l.name, i), raw_w)\n",
    "                # Concatenate the groups\n",
    "                output = tf.compat.v1.concat(values=output_groups, axis=3)\n",
    "                \n",
    "            \n",
    "            \n",
    "            self.add_end_points(l, output)\n",
    "        \n",
    "        elif l.type == 'ReLU' or l.type == 'PReLU':\n",
    "            bottom = self.get_layer(l.name, level='prev')[-1]\n",
    "            inputs = self.end_points[bottom.name]\n",
    "            output = tf.nn.relu(inputs, name=l.name)\n",
    "            self.add_end_points(l, output)\n",
    "        \n",
    "        elif l.type == 'InnerProduct':\n",
    "            params = l.inner_product_param\n",
    "            c_o = params.num_output\n",
    "            bias = params.bias_term\n",
    "            \n",
    "            bottom = self.get_layer(l.name, level='prev')[-1]\n",
    "            inputs = self.end_points[bottom.name]\n",
    "            input_shape = inputs.get_shape()\n",
    "            if len(input_shape) == 4:\n",
    "                n, h, w, c = input_shape \n",
    "                c_i = h*w*c\n",
    "                inputs = tf.reshape(inputs, shape=[1, c_i])\n",
    "            else:\n",
    "                n, c_i = input_shape\n",
    "            \n",
    "            raw_w = self.weights[l.name][0].data\n",
    "            raw_w = raw_w.transpose((1,0))\n",
    "            if bias:\n",
    "                raw_b = self.weights[l.name][1].data\n",
    "            else:\n",
    "                raw_b = np.zeros(c_o)\n",
    "            weights = self.make_tf_variable(l.name+'_fc_w', raw_w)\n",
    "            biases = self.make_tf_variable(l.name+'_fc_b', raw_b)\n",
    "            output = tf.compat.v1.nn.xw_plus_b(x=inputs, weights=weights, biases=biases, name=l.name)\n",
    "            self.add_end_points(l, output)\n",
    "            \n",
    "        elif l.type == 'Scale':\n",
    "            print(\"Merge into BN\")\n",
    "            \n",
    "        elif l.type == 'Eltwise':\n",
    "            bottoms = self.get_layer(l.name, level='prev')\n",
    "            output = self.end_points[bottoms[-1].name]\n",
    "            op = l.eltwise_param.operation\n",
    "            if op == 1:\n",
    "                for i, bottom in enumerate(bottoms[1:]):\n",
    "                    if bottom.type == 'BatchNorm':\n",
    "                        continue\n",
    "                    with tf.compat.v1.variable_scope(\"{}_add_{}\".format(l.name, i), values=[output]) as scope:\n",
    "                        output += self.end_points[bottom.name]\n",
    "            else:\n",
    "                print(\"Unsupported layer {}/{} at {}\".format(l.type, op, l.name))\n",
    "                sys.exit(1)\n",
    "            self.add_end_points(l, output)\n",
    "        elif l.type == 'Pooling':\n",
    "            bottoms = self.get_layer(l.name, level='prev')\n",
    "            inputs = self.end_points[bottoms[-1].name]\n",
    "            params = l.pooling_param\n",
    "            p_h = params.pad_h\n",
    "            p_w = params.pad_w\n",
    "            k_size = params.kernel_size\n",
    "            s = params.stride           \n",
    "            op = params.pool   \n",
    "            '''\n",
    "            enum PoolMethod {\n",
    "                MAX = 0;\n",
    "                AVE = 1;\n",
    "                STOCHASTIC = 2;\n",
    "              }\n",
    "            '''\n",
    "            if p_h > 0:\n",
    "                inputs = tf.pad(inputs, [[0, 0], [p_h, p_h], [p_w, p_w], [0, 0]], name=\"{}_pad\".format(l.name))\n",
    "                pad_type = 'VALID'\n",
    "            else:\n",
    "                if s > 1:\n",
    "                    pad_type = 'SAME'\n",
    "                else:\n",
    "                    pad_type = 'VALID'\n",
    "                    \n",
    "            if op == 0:\n",
    "                output = tf.nn.max_pool2d(inputs, ksize=[1, k_size, k_size, 1], strides=[1, s, s, 1], padding=pad_type, name=l.name)\n",
    "            if op == 1:\n",
    "                output = tf.nn.avg_pool2d(inputs, ksize=[1, k_size, k_size, 1], strides=[1, s, s, 1], padding=pad_type, name=l.name)\n",
    "                \n",
    "            self.add_end_points(l, output)\n",
    "            \n",
    "        elif l.type == 'Sigmoid':\n",
    "            bottoms = self.get_layer(l.name, level='prev')\n",
    "            inputs = self.end_points[bottoms[-1].name]\n",
    "            input_shape = inputs.get_shape()\n",
    "            if len(input_shape) == 4 and input_shape[1] == input_shape[2] == 1:\n",
    "                inputs = tf.reshape(inputs, shape=[input_shape[0], input_shape[3]], name=l.name+\"_flatten\")\n",
    "            output = tf.nn.sigmoid(inputs, name=l.name)\n",
    "            self.add_end_points(l, output)\n",
    "            \n",
    "        else:\n",
    "            print(\"Unsupported layer {} at {}\".format(l.type, l.name))\n",
    "            sys.exit(1)\n",
    "            \n",
    "    def build(self):\n",
    "        for layer in self.caffe_graph.layer:\n",
    "            self.add(layer)\n",
    "    \n",
    "    def allocate_weights(self):\n",
    "        for name in self.var_data_map.keys():\n",
    "            with tf.compat.v1.variable_scope(\"\", reuse=True):\n",
    "                data = self.var_data_map[name]\n",
    "                var = tf.compat.v1.get_variable(name)\n",
    "                self.sess.run(var.assign(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0911 11:24:10.737673 140472409773824 lazy_loader.py:50] \n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "slim = tf.contrib.slim\n",
    "\n",
    "img = tf.placeholder(np.float32, [1,7,7,3])\n",
    "conv_layer = slim.convolution2d(inputs=img, num_outputs=5, kernel_size=3, stride=1, padding='VALID', scope='test_conv',activation_fn=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0911 11:28:34.402771 140119288514304 lazy_loader.py:50] \n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "slim = tf.contrib.slim\n",
    "\n",
    "img = tf.placeholder(np.float32, [1,7,7,3])\n",
    "bn_layer = slim.batch_norm(inputs=img, center=True, scale=True, is_training=True, activation_fn=None, fused=True, scope='test_bn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'test_bn/FusedBatchNorm:0' shape=(1, 7, 7, 3) dtype=float32>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bn_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "slim = tf.contrib.slim\n",
    "\n",
    "img = tf.placeholder(np.float32, [1,3])\n",
    "fc_layer = slim.fully_connected(inputs=img, num_outputs=5, activation_fn=None, scope='test_fc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Placeholder                                   Placeholder         \n",
      "test_conv/weights/Initializer/random_uniform/shape Const               \n",
      "test_conv/weights/Initializer/random_uniform/min Const               \n",
      "test_conv/weights/Initializer/random_uniform/max Const               \n",
      "test_conv/weights/Initializer/random_uniform/RandomUniform RandomUniform       \n",
      "test_conv/weights/Initializer/random_uniform/sub Sub                 \n",
      "test_conv/weights/Initializer/random_uniform/mul Mul                 \n",
      "test_conv/weights/Initializer/random_uniform  Add                 \n",
      "test_conv/weights                             VariableV2          \n",
      "test_conv/weights/Assign                      Assign              \n",
      "test_conv/weights/read                        Identity            \n",
      "test_conv/biases/Initializer/zeros            Const               \n",
      "test_conv/biases                              VariableV2          \n",
      "test_conv/biases/Assign                       Assign              \n",
      "test_conv/biases/read                         Identity            \n",
      "test_conv/dilation_rate                       Const               \n",
      "test_conv/Conv2D                              Conv2D              \n",
      "test_conv/BiasAdd                             BiasAdd             \n",
      "Placeholder_1                                 Placeholder         \n",
      "test_bn/gamma/Initializer/ones                Const               \n",
      "test_bn/gamma                                 VariableV2          \n",
      "test_bn/gamma/Assign                          Assign              \n",
      "test_bn/gamma/read                            Identity            \n",
      "test_bn/beta/Initializer/zeros                Const               \n",
      "test_bn/beta                                  VariableV2          \n",
      "test_bn/beta/Assign                           Assign              \n",
      "test_bn/beta/read                             Identity            \n",
      "test_bn/moving_mean/Initializer/zeros         Const               \n",
      "test_bn/moving_mean                           VariableV2          \n",
      "test_bn/moving_mean/Assign                    Assign              \n",
      "test_bn/moving_mean/read                      Identity            \n",
      "test_bn/moving_variance/Initializer/ones      Const               \n",
      "test_bn/moving_variance                       VariableV2          \n",
      "test_bn/moving_variance/Assign                Assign              \n",
      "test_bn/moving_variance/read                  Identity            \n",
      "test_bn/Const                                 Const               \n",
      "test_bn/Const_1                               Const               \n",
      "test_bn/FusedBatchNorm                        FusedBatchNorm      \n",
      "test_bn/Const_2                               Const               \n",
      "test_bn/AssignMovingAvg/sub/x                 Const               \n",
      "test_bn/AssignMovingAvg/sub                   Sub                 \n",
      "test_bn/AssignMovingAvg/sub_1                 Sub                 \n",
      "test_bn/AssignMovingAvg/mul                   Mul                 \n",
      "test_bn/AssignMovingAvg                       AssignSub           \n",
      "test_bn/AssignMovingAvg_1/sub/x               Const               \n",
      "test_bn/AssignMovingAvg_1/sub                 Sub                 \n",
      "test_bn/AssignMovingAvg_1/sub_1               Sub                 \n",
      "test_bn/AssignMovingAvg_1/mul                 Mul                 \n",
      "test_bn/AssignMovingAvg_1                     AssignSub           \n",
      "Placeholder_2                                 Placeholder         \n",
      "Placeholder_3                                 Placeholder         \n",
      "test_fc/weights/Initializer/random_uniform/shape Const               \n",
      "test_fc/weights/Initializer/random_uniform/min Const               \n",
      "test_fc/weights/Initializer/random_uniform/max Const               \n",
      "test_fc/weights/Initializer/random_uniform/RandomUniform RandomUniform       \n",
      "test_fc/weights/Initializer/random_uniform/sub Sub                 \n",
      "test_fc/weights/Initializer/random_uniform/mul Mul                 \n",
      "test_fc/weights/Initializer/random_uniform    Add                 \n",
      "test_fc/weights                               VariableV2          \n",
      "test_fc/weights/Assign                        Assign              \n",
      "test_fc/weights/read                          Identity            \n",
      "test_fc/biases/Initializer/zeros              Const               \n",
      "test_fc/biases                                VariableV2          \n",
      "test_fc/biases/Assign                         Assign              \n",
      "test_fc/biases/read                           Identity            \n",
      "test_fc/MatMul                                MatMul              \n",
      "test_fc/BiasAdd                               BiasAdd             \n"
     ]
    }
   ],
   "source": [
    "graph = tf.get_default_graph()\n",
    "for i in graph.as_graph_def().node:\n",
    "    print(\"{:45} {:20}\".format(i.name, i.op))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "with tf.compat.v1.variable_scope(\"\", reuse=True):\n",
    "    data = np.ones((3,3,3,5))\n",
    "    var = tf.compat.v1.get_variable(\"test/weights\")\n",
    "    sess.run(var.assign(data))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
