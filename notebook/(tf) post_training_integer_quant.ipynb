{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_DDaAex5Q7u-"
   },
   "source": [
    "##### Copyright 2019 The TensorFlow Authors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "cellView": "form",
    "colab": {},
    "colab_type": "code",
    "id": "W1dWWdNHQ9L0"
   },
   "outputs": [],
   "source": [
    "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6Y8E0lw5eYWm"
   },
   "source": [
    "# Post-training integer quantization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CIGrZZPTZVeO"
   },
   "source": [
    "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://www.tensorflow.org/lite/performance/post_training_integer_quant\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\" />View on TensorFlow.org</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/performance/post_training_integer_quant.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/lite/g3doc/performance/post_training_integer_quant.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BTC1rDAuei_1"
   },
   "source": [
    "## Overview\n",
    "\n",
    "[TensorFlow Lite](https://www.tensorflow.org/lite/) now supports\n",
    "converting all model values (weights and activations) to 8-bit integers when converting from TensorFlow to TensorFlow Lite's flat buffer format. This results in a 4x reduction in model size and a 3 to 4x performance improvement on CPU performance. In addition, this fully quantized model can be consumed by integer-only hardware accelerators.\n",
    "\n",
    "In contrast to [post-training \"on-the-fly\" quantization](https://colab.sandbox.google.com/github/tensorflow/tensorflow/blob/master/tensorflow/lite/tutorials/post_training_quant.ipynb)—which stores only the weights as 8-bit integers—this technique statically quantizes all weights *and* activations during model conversion.\n",
    "\n",
    "In this tutorial, you'll train an MNIST model from scratch, check its accuracy in TensorFlow, and then convert the saved model into a Tensorflow Lite flatbuffer\n",
    "with full quantization. Finally, you'll check the\n",
    "accuracy of the converted model and compare it to the original float model.\n",
    "\n",
    "The training script, `mnist.py`, is available from the\n",
    "[TensorFlow official MNIST tutorial](https://github.com/tensorflow/models/tree/master/official/mnist).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2XsEP17Zelz9"
   },
   "source": [
    "## Build an MNIST model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dDqqUIZjZjac"
   },
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gyqAw1M9lyab"
   },
   "outputs": [],
   "source": [
    "! pip uninstall -y tensorflow\n",
    "! pip install -U tf-nightly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WsN6s5L1ieNl"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0904 09:42:06.971283 140354054096640 __init__.py:690] \n",
      "\n",
      "  TensorFlow's `tf-nightly` package will soon be updated to TensorFlow 2.0.\n",
      "\n",
      "  Please upgrade your code to TensorFlow 2.0:\n",
      "    * https://www.tensorflow.org/beta/guide/migration_guide\n",
      "\n",
      "  Or install the latest stable TensorFlow 1.X release:\n",
      "    * `pip install -U \"tensorflow==1.*\"`\n",
      "\n",
      "  Otherwise your code may be broken by the change.\n",
      "\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "00U0taBoe-w7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'models'...\n",
      "remote: Enumerating objects: 3235, done.\u001b[K\n",
      "remote: Counting objects: 100% (3235/3235), done.\u001b[K\n",
      "remote: Compressing objects: 100% (2734/2734), done.\u001b[K\n",
      "error: RPC failed; curl 56 GnuTLS recv error (-9): A TLS packet with unexpected length was received.\n",
      "fatal: The remote end hung up unexpectedly\n",
      "fatal: early EOF\n",
      "fatal: index-pack failed\n"
     ]
    }
   ],
   "source": [
    "! git clone --depth 1 https://github.com/tensorflow/models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4XZPtSh-fUOc"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "if sys.version_info.major >= 3:\n",
    "    import pathlib\n",
    "else:\n",
    "    import pathlib2 as pathlib\n",
    "\n",
    "# Add `models` to the python path.\n",
    "models_path = os.path.join(os.getcwd(), \"models\")\n",
    "sys.path.append(models_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/media/allen/mass/deep-learning-works/notebook'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eQ6Q0qqKZogR"
   },
   "source": [
    "### Train and export the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eMsw_6HujaqM"
   },
   "outputs": [],
   "source": [
    "saved_models_root = \"/tmp/mnist_saved_model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hWSAjQWagIHl"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\r\n",
      "W0904 09:44:51.894377 139847369615104 __init__.py:690] \r\n",
      "\r\n",
      "  TensorFlow's `tf-nightly` package will soon be updated to TensorFlow 2.0.\r\n",
      "\r\n",
      "  Please upgrade your code to TensorFlow 2.0:\r\n",
      "    * https://www.tensorflow.org/beta/guide/migration_guide\r\n",
      "\r\n",
      "  Or install the latest stable TensorFlow 1.X release:\r\n",
      "    * `pip install -U \"tensorflow==1.*\"`\r\n",
      "\r\n",
      "  Otherwise your code may be broken by the change.\r\n",
      "\r\n",
      "  \r\n",
      "Traceback (most recent call last):\r\n",
      "  File \"models/official/mnist/mnist.py\", line 24, in <module>\r\n",
      "    from official.mnist import dataset\r\n",
      "ImportError: No module named official.mnist\r\n"
     ]
    }
   ],
   "source": [
    "# The above path addition is not visible to subprocesses, add the path for the subprocess as well.\n",
    "# Note: channels_last is required here or the conversion may fail. \n",
    "!PYTHONPATH={models_path} python models/official/mnist/mnist.py --train_epochs=1 --export_dir {saved_models_root} --data_format=channels_last"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5NMaNZQCkW9X"
   },
   "source": [
    "This training won't take long because you're training the model for just a single epoch, which trains to about 96% accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xl8_fzVAZwOh"
   },
   "source": [
    "### Convert to a TensorFlow Lite model\n",
    "\n",
    "Using the [Python `TFLiteConverter`](https://www.tensorflow.org/lite/convert/python_api), you can now convert the trained model into a TensorFlow Lite model.\n",
    "\n",
    "The trained model is saved in the `saved_models_root` directory, which is named with a timestamp. So select the most recent directory: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Xp5oClaZkbtn"
   },
   "outputs": [],
   "source": [
    "saved_model_dir = str(sorted(pathlib.Path(saved_models_root).glob(\"*\"))[-1])\n",
    "saved_model_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AT8BgkKmljOy"
   },
   "source": [
    "Now load the model using the `TFLiteConverter`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_i8B2nDZmAgQ"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.enable_eager_execution()\n",
    "tf.logging.set_verbosity(tf.logging.DEBUG)\n",
    "\n",
    "converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)\n",
    "tflite_model = converter.convert()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "F2o2ZfF0aiCx"
   },
   "source": [
    "Write it out to a `.tflite` file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vptWZq2xnclo"
   },
   "outputs": [],
   "source": [
    "tflite_models_dir = pathlib.Path(\"/tmp/mnist_tflite_models/\")\n",
    "tflite_models_dir.mkdir(exist_ok=True, parents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ie9pQaQrn5ue"
   },
   "outputs": [],
   "source": [
    "tflite_model_file = tflite_models_dir/\"mnist_model.tflite\"\n",
    "tflite_model_file.write_bytes(tflite_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7BONhYtYocQY"
   },
   "source": [
    "Now you have a trained MNIST model that's converted to a `.tflite` file, but it's still using 32-bit float values for all parameter data.\n",
    "\n",
    "So let's convert the model again, this time using quantization...\n",
    "\n",
    "#### Convert using quantization\n",
    "First, first set the `optimizations` flag to optimize for size:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HEZ6ET1AHAS3"
   },
   "outputs": [],
   "source": [
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rTe8avZJHMDO"
   },
   "source": [
    "Now, in order to create quantized values with an accurate dynamic range of activations, you need to provide a representative dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FiwiWU3gHdkW"
   },
   "outputs": [],
   "source": [
    "mnist_train, _ = tf.keras.datasets.mnist.load_data()\n",
    "images = tf.cast(mnist_train[0], tf.float32)/255.0\n",
    "mnist_ds = tf.data.Dataset.from_tensor_slices((images)).batch(1)\n",
    "def representative_data_gen():\n",
    "  for input_value in mnist_ds.take(100):\n",
    "    yield [input_value]\n",
    "\n",
    "converter.representative_dataset = representative_data_gen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xW84iMYjHd9t"
   },
   "source": [
    "Finally, convert the model like usual. By default, the converted model will still use float input and outputs for invocation convenience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yuNfl3CoHNK3"
   },
   "outputs": [],
   "source": [
    "tflite_quant_model = converter.convert()\n",
    "tflite_model_quant_file = tflite_models_dir/\"mnist_model_quant.tflite\"\n",
    "tflite_model_quant_file.write_bytes(tflite_quant_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PhMmUTl4sbkz"
   },
   "source": [
    "Note how the resulting file is approximately `1/4` the size:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JExfcfLDscu4"
   },
   "outputs": [],
   "source": [
    "!ls -lh {tflite_models_dir}"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "post_training_integer_quant.ipynb",
   "private_outputs": true,
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
