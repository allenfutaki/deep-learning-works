{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"../\")\n",
    "import numpy as np\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from src.factory.config_factory import cfg, show_configs\n",
    "from src.factory.data_factory import DataFactory\n",
    "from src.factory.data_format_factory import DataFormatFactory\n",
    "from src.factory.transform_factory import TransformFactory\n",
    "from src.factory.loader_factory import LoaderFactory\n",
    "from src.base_data import BaseData\n",
    "from tools.logger import setup_logger\n",
    "logger = setup_logger(\"./external/\")\n",
    "\n",
    "from tools.scopehead_utils import scopehead_bbox_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg.NUM_WORKERS = 1\n",
    "cfg.DB.PATH = \"/media/acer/5f45949f-0fc7-4475-965b-e61989afcc10/Downloads/person_jde\"\n",
    "cfg.DB.DATA = \"cuhksysu\" #\"crowdhuman cuhksysu ethz prw caltech cityperson crowdhuman \"\n",
    "cfg.DB.DATA_FORMAT = \"coco\"\n",
    "# cfg.DB.TARGET_FORMAT = \"scopehead\"\n",
    "cfg.DB.LOADER = \"coco\"\n",
    "cfg.DB.USE_TRAIN = True\n",
    "cfg.DB.USE_TEST = True\n",
    "cfg.INPUT.SIZE = (512,512)\n",
    "cfg.INPUT.TRAIN_BS = 2\n",
    "cfg.INPUT.TEST_BS = 1\n",
    "cfg.MODEL.STRIDES = [1]\n",
    "# cfg.DB.TRAIN_TRANSFORM = \"RandScale RandomColorJitter-1.0-0.8-0.15-0.1-0.1 Tensorize\"\n",
    "# cfg.DB.TEST_TRANSFORM = \"ResizeKeepAspectRatio Tensorize\"\n",
    "cfg.REID.MSMT_ALL = False\n",
    "cfg.COCO.TARGET = 'person'\n",
    "# cfg.YOLO.ANCHORS = [6,16, 8,23, 11,32, 16,45,   21,64, 30,90, 43,128, 60,180,   85,255, 120,360, 170,420, 340, 320]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "loading annotations into memory...\nDone (t=1.25s)\ncreating index...\nindex created!\n2020-06-02 16:27:27,131 14489 coco.py INFO: => CUHKSYSU TRAIN is loaded\n2020-06-02 16:27:27,132 14489 coco.py INFO:   Dataset statistics:\n2020-06-02 16:27:27,133 14489 coco.py INFO:   -----------------------------\n2020-06-02 16:27:27,134 14489 coco.py INFO:   subset   | #id     | # images\n2020-06-02 16:27:27,139 14489 coco.py INFO:   -----------------------------\n2020-06-02 16:27:27,141 14489 coco.py INFO:   train    |   11934 |    18184\n2020-06-02 16:27:27,142 14489 coco.py INFO:   -----------------------------\n[]\n"
    }
   ],
   "source": [
    "data = DataFactory.produce(cfg, cfg.DB.PATH, cfg.DB.DATA, 'person', 0, 1, [1], None, True, False)\n",
    "trans = TransformFactory.produce(cfg, cfg.DB.TRAIN_TRANSFORM)\n",
    "dataset = DataFormatFactory.produce(cfg, data.train, trans, scopehead_bbox_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-fcc822b361d7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mbboxes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mgt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mbboxes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'bboxes'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/deep-learning-works/src/database/data_format/coco.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    128\u001b[0m                 \u001b[0mwh\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_w\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_h\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m                 \u001b[0mstrides\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrides\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m                 \u001b[0mids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalid_ids\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m             )\n\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/deep-learning-works/tools/scopehead_utils.py\u001b[0m in \u001b[0;36mscopehead_bbox_target\u001b[0;34m(cls_ids, bboxes, ids, max_objs, num_classes, out_sizes, num_bins, **kwargs)\u001b[0m\n\u001b[1;32m     75\u001b[0m                 \u001b[0mbins\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0munit_h\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m                 \u001b[0mbins\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0munit_w\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m                 \u001b[0mbins\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0munit_h\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m                 \u001b[0mreg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m2\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mct\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mct_int\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0munit_w\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0munit_w\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m                 \u001b[0mreg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m2\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mct\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mct_int\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0munit_h\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0munit_h\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "bboxes = []\n",
    "for gt in dataset:\n",
    "    bboxes.append(gt['bboxes'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = LoaderFactory.produce(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def rotate_A(level, shape, **kwargs):\n",
    "    # copy from https://pillow.readthedocs.io/en/stable/_modules/PIL/Image.html#Image.rotate\n",
    "    angle = level % 360.0\n",
    "\n",
    "    w, h = shape\n",
    "\n",
    "    post_trans = (0, 0)\n",
    "    rotn_center = (w / 2.0, h / 2.0)\n",
    "    angle = math.radians(angle)\n",
    "    matrix = [\n",
    "        round(math.cos(angle), 15),\n",
    "        round(math.sin(angle), 15),\n",
    "        0.0,\n",
    "        round(-math.sin(angle), 15),\n",
    "        round(math.cos(angle), 15),\n",
    "        0.0,\n",
    "    ]\n",
    "\n",
    "    def transform(x, y, matrix):\n",
    "        (a, b, c, d, e, f) = matrix\n",
    "        return a * x + b * y + c, d * x + e * y + f\n",
    "\n",
    "    matrix[2], matrix[5] = transform(\n",
    "        -rotn_center[0] - post_trans[0], -rotn_center[1] - post_trans[1], matrix\n",
    "    )\n",
    "    matrix[2] += rotn_center[0]\n",
    "    matrix[5] += rotn_center[1]\n",
    "\n",
    "    return np.array(matrix).reshape(2, 3).astype(np.float)\n",
    "def apply_A(pt, A):\n",
    "    new_pt = np.array([pt[0], pt[1], 1.], dtype=np.float32).T\n",
    "    new_pt = np.dot(A, new_pt)\n",
    "    return new_pt[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tools.utils import _tranpose_and_gather_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(loader['train']))\n",
    "wh = batch[(512,512)]['wh']\n",
    "ind = batch[(512,512)]['ind']\n",
    "reg_mask = batch[(512,512)]['reg_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = torch.zeros(2,5*2,512,512)\n",
    "output[:,5:7,...] = 1\n",
    "output[:,:3,...] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "tensor([5., 5.])"
     },
     "metadata": {},
     "execution_count": 107
    }
   ],
   "source": [
    "output[0,:,0,0].new_tensor([5,5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = _tranpose_and_gather_feat(output, ind)\n",
    "pred_mask = reg_mask.unsqueeze(2).expand_as(pred).bool()\n",
    "wh_mask = reg_mask.unsqueeze(2).expand_as(wh).bool()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "torch.Size([1, 100, 1])"
     },
     "metadata": {},
     "execution_count": 109
    }
   ],
   "source": [
    "a = torch.rand(1,100,1)\n",
    "b = torch.rand(1,100,1)\n",
    "(a*b).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordinal_wh = pred.view(2, -1, 2, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "tensor([[1., 1., 1., 0., 0.],\n        [1., 1., 0., 0., 0.]])"
     },
     "metadata": {},
     "execution_count": 102
    }
   ],
   "source": [
    "ordinal_wh[0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(ordinal_wh >= 0.5).sum(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wh_mask.sum(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordinal_wh = pred.view(2, 100, 2, 5, 2)\n",
    "rank = ordinal_wh.max(dim=-1)[1].sum(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rank.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "crit = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred[pred_mask].reshape(-1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wh[0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(wh * wh_mask).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred[pred_mask].reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "tensor([1, 0, 0, 0, 0, 1, 1, 0, 0, 0])"
     },
     "metadata": {},
     "execution_count": 74
    }
   ],
   "source": [
    "(wh * wh_mask).view(-1)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "torch.Size([500])"
     },
     "metadata": {},
     "execution_count": 79
    }
   ],
   "source": [
    "pred[pred_mask].view(-1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_mask[...] = False\n",
    "wh_mask[...] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "tensor(nan)"
     },
     "metadata": {},
     "execution_count": 93
    }
   ],
   "source": [
    "crit(pred[pred_mask].view(-1), wh[wh_mask].view(-1).float())# / (wh_mask.sum() + 1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "tensor(2.9641)"
     },
     "metadata": {},
     "execution_count": 84
    }
   ],
   "source": [
    "crit((pred * pred_mask).view(-1), (wh * wh_mask).view(-1).float()) / wh_mask.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "tensor([0.5000])"
     },
     "metadata": {},
     "execution_count": 82
    }
   ],
   "source": [
    "torch.sigmoid(torch.Tensor([0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wh[mask.sum(dim=-1).bool()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred[mask].reshape(1,-1,4,5).max(dim=-1)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_names = cfg.DB.DATA.split(\" \")\n",
    "handles = []\n",
    "indice = []\n",
    "pids = []\n",
    "offset = 0\n",
    "for idx, name in enumerate(data_names):\n",
    "    data = DataFactory.produce(cfg, data_name=name)\n",
    "    handles.append(data.train['handle'])\n",
    "    pids.append(data.train['pid'])\n",
    "    for img_id, img_path in data.train['indice']:\n",
    "        indice.append((img_id, img_path, idx, offset))\n",
    "    offset += (max(list(data.train['pid'].values())) + 1)\n",
    "\n",
    "_data = BaseData()\n",
    "_data.train['handle'] = handles\n",
    "_data.train['indice'] = indice\n",
    "_data.train['pid'] = pids\n",
    "_data.train['strides'] = cfg.MODEL.STRIDES\n",
    "_data.train['num_classes'] = offset\n",
    "_data.train['num_keypoints'] = 0\n",
    "_data.train['num_person'] = 0\n",
    "cfg.DB.NUM_CLASSES = offset\n",
    "\n",
    "trans = TransformFactory.produce(cfg, cfg.DB.TRAIN_TRANSFORM)\n",
    "dataset = DataFormatFactory.produce(cfg, _data.train, trans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = LoaderFactory.produce(cfg)\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pycocotools.coco as coco\n",
    "import json\n",
    "import os\n",
    "import os.path as osp\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import shutil\n",
    "import cv2\n",
    "\n",
    "from tools.yolov3_utils import decode_delta_map\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(loader['train']))\n",
    "batch['bboxes']\n",
    "bboxes = np.vstack(batch['bboxes'])\n",
    "bboxes[:, [0,2]] *= 576\n",
    "bboxes[:, [1,3]] *= 320"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 0\n",
    "batch = next(iter(loader['train']))\n",
    "np_img = (batch['inp'][idx]*255).permute(1,2,0).numpy().astype(np.uint8)\n",
    "h, w = np_img.shape[:2]\n",
    "bboxes = np.vstack(batch['bboxes'][idx])\n",
    "bboxes[:, [0,2]] *= w\n",
    "bboxes[:, [1,3]] *= h\n",
    "np_img = cv2.resize(np_img, (w, h))\n",
    "Image.fromarray(np_img)\n",
    "for b in bboxes:\n",
    "    x1, y1, x2, y2 = b\n",
    "    cv2.rectangle(np_img, (int(x1), int(y1)), (int(x2), int(y2)), (255,255,0), 2)\n",
    "Image.fromarray(np_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shapes = ['72x40', '36x20', '18x10']\n",
    "anchors = torch.Tensor(cfg.YOLO.ANCHORS).view(len(cfg.MODEL.STRIDES), -1, 2)\n",
    "for stage, shape in enumerate(shapes):\n",
    "    mask = batch[f\"yolov3_{shape}_t_conf\"] > 0\n",
    "    t_bbox = batch[f\"yolov3_{shape}_t_bbox\"]\n",
    "    print(mask.sum(), shape)\n",
    "    target_anchors = anchors[stage] / cfg.MODEL.STRIDES[stage]\n",
    "    p_bbox = decode_delta_map(t_bbox, target_anchors)[mask] * cfg.MODEL.STRIDES[stage]\n",
    "    p_bbox[:,0] -= p_bbox[:,2] / 2\n",
    "    p_bbox[:,1] -= p_bbox[:,3] / 2\n",
    "    for b in p_bbox:\n",
    "        x1, y1, w, h = b\n",
    "        cv2.rectangle(np_img, (int(x1), int(y1)), (int(x1+w), int(y1+h)), (0,0,225//(stage+1)), 2)\n",
    "Image.fromarray(np_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.6.8-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python36864bitpy368venv96aa759879e8443787e614586faca138",
   "display_name": "Python 3.6.8 64-bit ('py368': venv)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}