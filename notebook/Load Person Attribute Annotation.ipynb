{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os, re\n",
    "from collections import OrderedDict\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "from scipy.io import loadmat\n",
    "import itertools\n",
    "import json\n",
    "from scipy.stats import chi2_contingency\n",
    "from scipy.stats import chi2\n",
    "os.chdir(\"../../PA_labels/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PARSE27K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "pathToDataset = \"parse27k\"\n",
    "\n",
    "attributes = ALL_ATTRIBUTES = ('Orientation',\n",
    "                               'Orientation8',\n",
    "                               'Gender',\n",
    "                               'Posture',\n",
    "                               'HasBagOnShoulderLeft', 'HasBagOnShoulderRight',\n",
    "                               'HasBagInHandLeft', 'HasBagInHandRight',\n",
    "                               'HasTrolley',\n",
    "                               'HasBackpack',\n",
    "                               'isPushing',\n",
    "                               'isTalkingOnPhone')\n",
    "\n",
    "TRAIN_SEQUENCES = (1, 4, 5)\n",
    "VAL_SEQUENCES = (2, 7, 8)\n",
    "TEST_SEQUENCES = (3, 6)\n",
    "TRAINVAL_SEQUENCES = (1, 4, 5, 2, 7, 8)\n",
    "ALL_SEQUENCES = (1, 2, 3, 4, 5, 6, 7, 8)\n",
    "\n",
    "split = 'test'\n",
    "\n",
    "def _translate_db_label_to_softmax(attribute, label):\n",
    "    \"\"\"\n",
    "    translates a label from the sqlite database to a softmax label.\n",
    "    The softmax range is (0,1,...,N) - where 0 is the N/A label.\n",
    "    (0=NA, 1=POS, 2=NEG)\n",
    "    (0=NA, 1=front, 2=back, 3=left, 4=right)\n",
    "    \"\"\"\n",
    "    msg = 'unexpected label - attribute: {} - value: {}'\n",
    "    if not isinstance(label, int):\n",
    "        raise TypeError('label expected to be integer')\n",
    "    if not attribute in attributes:\n",
    "        raise ValueError('invalid attribute')\n",
    "\n",
    "    # translate to range [0,1,..N]\n",
    "    # by convention we handled the male as the 'pos' label\n",
    "    # this can have an influence on the exact value of AP scores\n",
    "    if attribute == 'Posture': \n",
    "        if label == 3: # standing -> pos (less frequent)\n",
    "            out = 1\n",
    "        elif label == 2: #walking -> neg (the more frequent class)\n",
    "            out = 2\n",
    "        elif label == 1:\n",
    "            out = 0\n",
    "        else:\n",
    "            raise ValueError(msg.format(attribute, label))\n",
    "    else:\n",
    "        out = label - 1\n",
    "    return out\n",
    "\n",
    "def _translate_db_labels_to_softmax(labels):\n",
    "    \"\"\"\n",
    "    applies translation all attributes\n",
    "    - should be useful when we support working only on a subset of\n",
    "     attributes\n",
    "    \"\"\"\n",
    "    out_labels = []\n",
    "    if len(attributes) != len(labels):\n",
    "        msg = 'length of labels does not match my attribute count!'\n",
    "        raise ValueError(msg)\n",
    "\n",
    "    out_labels = [_translate_db_label_to_softmax(a, l) for (a, l)\n",
    "                  in zip(attributes, labels)]\n",
    "    return out_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    dbFile = os.path.join(pathToDataset, \"annotations.sqlite3\")\n",
    "    db = sqlite3.connect(dbFile)\n",
    "    dbc = db.cursor()\n",
    "except sqlite3.Error as e:\n",
    "    raise Exception(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "            SELECT s.directory as directory,\n",
      "                   i.filename as filename,\n",
      "                   p.pedestrianID as pid,\n",
      "                   p.box_min_x as min_x, p.box_min_y as min_y,\n",
      "                   p.box_max_x as max_x, p.box_max_y as max_y,\n",
      "                   OrientationID, Orientation8ID, GenderID, PostureID, HasBagOnShoulderLeftID, HasBagOnShoulderRightID, HasBagInHandLeftID, HasBagInHandRightID, HasTrolleyID, HasBackpackID, isPushingID, isTalkingOnPhoneID\n",
      "            FROM Pedestrian p\n",
      "            INNER JOIN AttributeSet a ON p.attributeSetID = a.attributeSetID\n",
      "            INNER JOIN Image i ON p.imageID = i.imageID\n",
      "            INNER JOIN Sequence s on s.sequenceID = i.sequenceID\n",
      "         WHERE a.postureID <> 4 AND i.sequenceID IN (3, 6)\n"
     ]
    }
   ],
   "source": [
    "if split == 'train':\n",
    "    sequenceIDs = TRAINVAL_SEQUENCES\n",
    "elif split == 'val':\n",
    "    sequenceIDs = VAL_SEQUENCES\n",
    "elif split == 'test':\n",
    "    sequenceIDs = TEST_SEQUENCES\n",
    "            \n",
    "query = '''\n",
    "            SELECT s.directory as directory,\n",
    "                   i.filename as filename,\n",
    "                   p.pedestrianID as pid,\n",
    "                   p.box_min_x as min_x, p.box_min_y as min_y,\n",
    "                   p.box_max_x as max_x, p.box_max_y as max_y,\n",
    "                   {0}\n",
    "            FROM Pedestrian p\n",
    "            INNER JOIN AttributeSet a ON p.attributeSetID = a.attributeSetID\n",
    "            INNER JOIN Image i ON p.imageID = i.imageID\n",
    "            INNER JOIN Sequence s on s.sequenceID = i.sequenceID\n",
    "        '''.format(', '.join((a+'ID' for a in attributes)))\n",
    "\n",
    "query += ' WHERE a.postureID <> 4 ' # filter out all 'sitting' examples\n",
    "query += 'AND i.sequenceID IN ' + str(sequenceIDs)\n",
    "\n",
    "print(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = dbc.execute(query).fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = []\n",
    "for row in results:\n",
    "    fullFileName = os.path.join(pathToDataset, 'sequences', row[0], row[1])\n",
    "    box = tuple(row[3:7])\n",
    "    pid = str(row[2])\n",
    "    labels = _translate_db_labels_to_softmax(row[7:])\n",
    "    temp = [fullFileName, pid, box] + labels\n",
    "    p.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data=p, columns=['path', 'pid', 'bbox'] + list(attributes))\n",
    "df.to_csv(\"./parse27k/parse27k_test_labels.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PA-100K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dict()\n",
    "dataset['description'] = 'pa100k'\n",
    "dataset['root'] = './dataset/pa100k/data/'\n",
    "dataset['image'] = []\n",
    "dataset['att'] = []\n",
    "dataset['att_name'] = []\n",
    "dataset['selected_attribute'] = range(26)\n",
    "# load ANNOTATION.MAT\n",
    "data = loadmat('./PA-100K_anno/annotation.mat')\n",
    "for idx in range(26):\n",
    "    dataset['att_name'].append(data['attributes'][idx][0][0])\n",
    "\n",
    "for idx in range(80000):\n",
    "    dataset['image'].append(data['train_images_name'][idx][0][0])\n",
    "    dataset['att'].append(data['train_label'][idx, :].tolist())\n",
    "\n",
    "for idx in range(10000):\n",
    "    dataset['image'].append(data['val_images_name'][idx][0][0])\n",
    "    dataset['att'].append(data['val_label'][idx, :].tolist())\n",
    "\n",
    "for idx in range(10000):\n",
    "    dataset['image'].append(data['test_images_name'][idx][0][0])\n",
    "    dataset['att'].append(data['test_label'][idx, :].tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = []\n",
    "for i in range(len(dataset['image'])):\n",
    "    path = dataset['image'][i]\n",
    "    att = dataset['att'][i]\n",
    "    p.append([path] + att)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data=p, columns=['path'] + dataset['att_name'])\n",
    "df.to_csv(\"./PA-100K_anno/PA-100K_labels.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "pa100k = pd.read_csv(\"./PA-100K_anno/PA-100K_labels.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['path', 'Female', 'AgeOver60', 'Age18-60', 'AgeLess18', 'Front', 'Side',\n",
       "       'Back', 'Hat', 'Glasses', 'HandBag', 'ShoulderBag', 'Backpack',\n",
       "       'HoldObjectsInFront', 'ShortSleeve', 'LongSleeve', 'UpperStride',\n",
       "       'UpperLogo', 'UpperPlaid', 'UpperSplice', 'LowerStripe', 'LowerPattern',\n",
       "       'LongCoat', 'Trousers', 'Shorts', 'Skirt&Dress', 'boots'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pa100k.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "path                     :      0 < 10000\n",
      "Female                   :  45336\n",
      "AgeOver60                :   1469 < 10000\n",
      "Age18-60                 :  92844\n",
      "AgeLess18                :   5687 < 10000\n",
      "Front                    :  34707\n",
      "Side                     :  30508\n",
      "Back                     :  34785\n",
      "Hat                      :   4206 < 10000\n",
      "Glasses                  :  18662\n",
      "HandBag                  :  18115\n",
      "ShoulderBag              :  19301\n",
      "Backpack                 :  15926\n",
      "HoldObjectsInFront       :    958 < 10000\n",
      "ShortSleeve              :  56913\n",
      "LongSleeve               :  43087\n",
      "UpperStride              :   5088 < 10000\n",
      "UpperLogo                :  14835\n",
      "UpperPlaid               :  10917\n",
      "UpperSplice              :   4219 < 10000\n",
      "LowerStripe              :    450 < 10000\n",
      "LowerPattern             :   1639 < 10000\n",
      "LongCoat                 :   3365 < 10000\n",
      "Trousers                 :  71916\n",
      "Shorts                   :  16896\n",
      "Skirt&Dress              :  11155\n",
      "boots                    :    595 < 10000\n"
     ]
    }
   ],
   "source": [
    "for col in pa100k.columns:\n",
    "    count = pa100k[col][pa100k[col]==1].count()\n",
    "    if count < 10000:\n",
    "        print(\"{:25}: {:6} < 10000\".format(col, count))\n",
    "    else:\n",
    "        print(\"{:25}: {:6}\".format(col, count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4206"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pa100k['Hat'][pa100k['Hat']==1].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_table = pd.crosstab(pa100k['Front'], pa100k['Hat'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Hat</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Front</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>62381</td>\n",
       "      <td>2912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>33413</td>\n",
       "      <td>1294</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Hat        0     1\n",
       "Front             \n",
       "0      62381  2912\n",
       "1      33413  1294"
      ]
     },
     "execution_count": 283,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_table.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[623,  29],\n",
       "       [334,  12]])"
      ]
     },
     "execution_count": 320,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = (df_table.values/100).astype(int)\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[100,  20],\n",
       "       [  6,  90]])"
      ]
     },
     "execution_count": 327,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = np.array([[100, 20],[6,  90]])\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.617816970520597e-29"
      ]
     },
     "execution_count": 328,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stat, p, dof, expected = chi2_contingency(test)\n",
    "p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PETA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = ['Black', 'Blue', 'Brown', 'Green', 'Grey', 'Orange', 'Pink', 'Purple', 'Red', 'White', 'Yellow']\n",
    "part = ['upperBody', 'lowerBody', 'footwear', 'hair']\n",
    "part_colors = [pc[0]+pc[1] for pc in itertools.product(part, colors)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "attributes = dict()\n",
    "with open(\"PETA/attribute.txt\", 'r') as f:\n",
    "    for i, line in enumerate(f):\n",
    "        line = line.strip()\n",
    "        attributes[line] = i\n",
    "for i, pc in enumerate(part_colors):\n",
    "    attributes[pc] = i + 61\n",
    "attributes_str = json.dumps(attributes)\n",
    "with open(\"PETA/attributes.txt\", 'w') as f:\n",
    "    f.write(attributes_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_path = [os.path.join(root, f) for root, _, files in os.walk(\"PETA/\") for f in files if 'Label' in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['PETA/PETA dataset/PRID/archive/Label.txt',\n",
       " 'PETA/PETA dataset/GRID/archive/Label.txt',\n",
       " 'PETA/PETA dataset/SARC3D/archive/Label.txt',\n",
       " 'PETA/PETA dataset/VIPeR/archive/Label.txt',\n",
       " 'PETA/PETA dataset/TownCentre/archive/Label.txt',\n",
       " 'PETA/PETA dataset/3DPeS/archive/Label.txt',\n",
       " 'PETA/PETA dataset/MIT/archive/Label.txt',\n",
       " 'PETA/PETA dataset/CAVIAR4REID/archive/Label.txt',\n",
       " 'PETA/PETA dataset/CUHK/archive/Label.txt',\n",
       " 'PETA/PETA dataset/i-LID/archive/Label.txt']"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lowerBodyLogo\n",
      "accessoryShawl\n",
      "lowerBodyLogo\n",
      "accessoryShawl\n",
      "accessoryFaceMask\n",
      "accessoryShawl\n",
      "lowerBodyLogo\n",
      "lowerBodyLogo\n",
      "lowerBodyLogo\n",
      "accessoryFaceMask\n",
      "lowerBodyLogo\n",
      "accessoryShawl\n",
      "accessoryFaceMask\n",
      "accessoryFaceMask\n",
      "lowerBodyLogo\n",
      "accessoryFaceMask\n",
      "accessoryFaceMask\n",
      "accessoryShawl\n",
      "accessoryShawl\n",
      "lowerBodyLogo\n",
      "accessoryFaceMask\n",
      "accessoryFaceMask\n"
     ]
    }
   ],
   "source": [
    "for labels in label_path:\n",
    "    branch = labels.split(\"/\")[2]\n",
    "    raw_df = pd.read_csv(labels, header=None)\n",
    "\n",
    "    p = []\n",
    "    for index in raw_df.index:\n",
    "        raw_labels = raw_df.loc[index][0].split(\" \")\n",
    "        pid = int(raw_labels[0].split(\".\")[0])\n",
    "        atts = []\n",
    "        code = np.zeros(len(attributes), dtype=int)\n",
    "        for raw_label in raw_labels[1:]:\n",
    "            try:\n",
    "                atts.append(attributes[raw_label])            \n",
    "            except:\n",
    "                print(raw_label)\n",
    "                continue\n",
    "        code[atts] = 1\n",
    "        p.append([pid]+code.tolist())\n",
    "\n",
    "    df = pd.DataFrame(p, columns=['pid'] + list(attributes.keys()))\n",
    "    df.to_csv(\"./PETA/{}_labels.csv\".format(branch))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Duke"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_DukeMTMCAttribute(dataset_dir):\n",
    "    if not os.path.exists(os.path.join(dataset_dir)):\n",
    "        print('Please Download the DukeMTMCATTributes Dataset')\n",
    "    train_label = ['backpack',\n",
    "                   'bag',\n",
    "                   'handbag',\n",
    "                   'boots',\n",
    "                   'gender',\n",
    "                   'hat',\n",
    "                   'shoes',\n",
    "                   'top',\n",
    "                   'downblack',\n",
    "                   'downwhite',\n",
    "                   'downred',\n",
    "                   'downgray',\n",
    "                   'downblue',\n",
    "                   'downgreen',\n",
    "                   'downbrown',\n",
    "                   'upblack',\n",
    "                   'upwhite',\n",
    "                   'upred',\n",
    "                   'uppurple',\n",
    "                   'upgray',\n",
    "                   'upblue',\n",
    "                   'upgreen',\n",
    "                   'upbrown']\n",
    "    \n",
    "    test_label=['boots',\n",
    "                'shoes',\n",
    "                'top',\n",
    "                'gender',\n",
    "                'hat',\n",
    "                'backpack',\n",
    "                'bag',\n",
    "                'handbag',\n",
    "                'downblack',\n",
    "                'downwhite',\n",
    "                'downred',\n",
    "                'downgray',\n",
    "                'downblue',\n",
    "                'downgreen',\n",
    "                'downbrown',\n",
    "                'upblack',\n",
    "                'upwhite',\n",
    "                'upred',\n",
    "                'upgray',\n",
    "                'upblue',\n",
    "                'upgreen',\n",
    "                'uppurple',\n",
    "                'upbrown']  \n",
    "    \n",
    "    f = loadmat(os.path.join(dataset_dir,'duke_attribute.mat'))\n",
    "    \n",
    "    train_person_id = []\n",
    "    for personid in f['duke_attribute'][0][0][0][0][0][-1].squeeze().tolist():\n",
    "        train_person_id.append(int(personid))\n",
    "    train_person_id.sort(key=int)\n",
    "\n",
    "    test_person_id = []\n",
    "    for personid in f['duke_attribute'][0][0][1][0][0][-1].squeeze().tolist():\n",
    "        test_person_id.append(int(personid))\n",
    "    test_person_id.sort(key=int)\n",
    "    \n",
    "    test_attribute = {}\n",
    "    train_attribute = {}\n",
    "    for test_train in range(len(f['duke_attribute'][0][0])):\n",
    "        if test_train == 1:\n",
    "            id_list_name = 'test_person_id'\n",
    "            group_name = 'test_attribute'\n",
    "        else:\n",
    "            id_list_name = 'train_person_id'\n",
    "            group_name = 'train_attribute'\n",
    "        for attribute_id in range(len(f['duke_attribute'][0][0][test_train][0][0])):\n",
    "            if isinstance(f['duke_attribute'][0][0][test_train][0][0][attribute_id][0][0], np.ndarray):\n",
    "                continue\n",
    "            for person_id in range(len(f['duke_attribute'][0][0][test_train][0][0][attribute_id][0])):\n",
    "                id = locals()[id_list_name][person_id]\n",
    "                if id not in locals()[group_name]:\n",
    "                    locals()[group_name][id]=[]\n",
    "                locals()[group_name][id].append(f['duke_attribute'][0][0][test_train][0][0][attribute_id][0][person_id])\n",
    "    \n",
    "    for i in range(8):\n",
    "        train_label.insert(8,train_label[-1])\n",
    "        train_label.pop(-1)\n",
    "    \n",
    "    unified_train_atr = {}\n",
    "    for k,v in train_attribute.items():\n",
    "        temp_atr = list(v)\n",
    "        for i in range(8):\n",
    "            temp_atr.insert(8,temp_atr[-1])\n",
    "            temp_atr.pop(-1)\n",
    "        unified_train_atr[k] = temp_atr\n",
    "    \n",
    "    unified_test_atr = {}\n",
    "    for k,v in test_attribute.items():\n",
    "        temp_atr = [0]*len(train_label)\n",
    "        for i in range(len(train_label)):\n",
    "            temp_atr[i]=v[test_label.index(train_label[i])]\n",
    "        unified_test_atr[k] = temp_atr\n",
    "    #two zero appear in train '0370' '0679'\n",
    "    #zero_check=[]\n",
    "    #for id in train_attribute:\n",
    "    #    if 0 in train_attribute[id]:\n",
    "    #        zero_check.append(id)\n",
    "    #for i in range(len(zero_check)):\n",
    "    #    train_attribute[zero_check[i]] = [1 if x==0 else x for x in train_attribute[zero_check[i]]]\n",
    "    unified_train_atr[370][7]=1\n",
    "    unified_train_atr[679][7]=2\n",
    "\n",
    "    return unified_train_atr,unified_test_atr,train_label\n",
    "\n",
    "def import_DukeMTMCAttribute_binary(dataset_dir):\n",
    "\ttrain_duke_attr, test_duke_attr,label = import_DukeMTMCAttribute(dataset_dir)\n",
    "\tfor id in train_duke_attr:\n",
    "\t\ttrain_duke_attr[id][:] = [x - 1 for x in train_duke_attr[id]]\n",
    "\tfor id in test_duke_attr:\n",
    "\t\ttest_duke_attr[id][:] = [x - 1 for x in test_duke_attr[id]]\n",
    "\treturn train_duke_attr, test_duke_attr, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_duke_attr, test_duke_attr, label = import_DukeMTMCAttribute_binary(\"DukeMTMC-attribute/\")\n",
    "\n",
    "p = []\n",
    "for pid in train_duke_attr.keys():\n",
    "    p.append([pid] + train_duke_attr[pid])\n",
    "\n",
    "df = pd.DataFrame(p, columns=['pid'] + label)\n",
    "df.to_csv(\"./DukeMTMC-attribute/train_Duke_labels.csv\")\n",
    "\n",
    "p = []\n",
    "for pid in test_duke_attr.keys():\n",
    "    p.append([pid] + test_duke_attr[pid])\n",
    "\n",
    "df = pd.DataFrame(p, columns=['pid'] + label)\n",
    "df.to_csv(\"./DukeMTMC-attribute/test_Duke_labels.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Market"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_Market1501Attribute(dataset_dir):\n",
    "    if not os.path.exists(os.path.join(dataset_dir)):\n",
    "        print('Please Download the Market1501Attribute Dataset')\n",
    "    train_label=['age',\n",
    "           'backpack',\n",
    "           'bag',\n",
    "           'handbag',\n",
    "           'downblack',\n",
    "           'downblue',\n",
    "           'downbrown',\n",
    "           'downgray',\n",
    "           'downgreen',\n",
    "           'downpink',\n",
    "           'downpurple',\n",
    "           'downwhite',\n",
    "           'downyellow',\n",
    "           'upblack',\n",
    "           'upblue',\n",
    "           'upgreen',\n",
    "           'upgray',\n",
    "           'uppurple',\n",
    "           'upred',\n",
    "           'upwhite',\n",
    "           'upyellow',\n",
    "           'clothes',\n",
    "           'down',\n",
    "           'up',\n",
    "           'hair',\n",
    "           'hat',\n",
    "           'gender']\n",
    "    \n",
    "    test_label=['age',\n",
    "           'backpack',\n",
    "           'bag',\n",
    "           'handbag',\n",
    "           'clothes',\n",
    "           'down',\n",
    "           'up',\n",
    "           'hair',\n",
    "           'hat',\n",
    "           'gender',\n",
    "           'upblack',\n",
    "           'upwhite',\n",
    "           'upred',\n",
    "           'uppurple',\n",
    "           'upyellow',\n",
    "           'upgray',\n",
    "           'upblue',\n",
    "           'upgreen',\n",
    "           'downblack',\n",
    "           'downwhite',\n",
    "           'downpink',\n",
    "           'downpurple',\n",
    "           'downyellow',\n",
    "           'downgray',\n",
    "           'downblue',\n",
    "           'downgreen',\n",
    "           'downbrown'\n",
    "           ]  \n",
    "\n",
    "    f = scipy.io.loadmat(os.path.join(dataset_dir,'market_attribute.mat'))\n",
    "    \n",
    "    train_person_id = []\n",
    "    for personid in f['market_attribute'][0][0][1][0][0][-1].squeeze().tolist():\n",
    "        train_person_id.append(int(personid))\n",
    "    train_person_id.sort(key=int)\n",
    "\n",
    "    test_person_id = []\n",
    "    for personid in f['market_attribute'][0][0][0][0][0][-1].squeeze().tolist():\n",
    "        test_person_id.append(int(personid))\n",
    "    test_person_id.sort(key=int)\n",
    "#     test_person_id.remove('-1')\n",
    "#     test_person_id.remove('0000')\n",
    "    \n",
    "    test_attribute = {}\n",
    "    train_attribute = {}\n",
    "    for test_train in range(len(f['market_attribute'][0][0])):\n",
    "        if test_train == 0:\n",
    "            id_list_name = 'test_person_id'\n",
    "            group_name = 'test_attribute'\n",
    "        else:\n",
    "            id_list_name = 'train_person_id'\n",
    "            group_name = 'train_attribute'\n",
    "        for attribute_id in range(len(f['market_attribute'][0][0][test_train][0][0])):\n",
    "            if isinstance(f['market_attribute'][0][0][test_train][0][0][attribute_id][0][0], np.ndarray):\n",
    "                continue\n",
    "            for person_id in range(len(f['market_attribute'][0][0][test_train][0][0][attribute_id][0])):\n",
    "                id = locals()[id_list_name][person_id]              \n",
    "                if id not in locals()[group_name]:\n",
    "                    locals()[group_name][id]=[]\n",
    "                locals()[group_name][id].append(f['market_attribute'][0][0][test_train][0][0][attribute_id][0][person_id])\n",
    "    \n",
    "    unified_train_atr = {}\n",
    "    for k,v in train_attribute.items():\n",
    "        temp_atr = [0]*len(test_label)\n",
    "        for i in range(len(test_label)):\n",
    "            temp_atr[i]=v[train_label.index(test_label[i])]\n",
    "        unified_train_atr[k] = temp_atr\n",
    "    \n",
    "    return unified_train_atr, test_attribute, test_label\n",
    "\n",
    "\n",
    "def import_Market1501Attribute_binary(dataset_dir):\n",
    "    train_market_attr, test_market_attr, label = import_Market1501Attribute(dataset_dir)\n",
    "    \n",
    "    for id in train_market_attr:\n",
    "        train_market_attr[id][:] = [x - 1 for x in train_market_attr[id]]\n",
    "        if train_market_attr[id][0] == 0:\n",
    "            train_market_attr[id].pop(0)\n",
    "            train_market_attr[id].insert(0, 1)\n",
    "            train_market_attr[id].insert(1, 0)\n",
    "            train_market_attr[id].insert(2, 0)\n",
    "            train_market_attr[id].insert(3, 0)\n",
    "        elif train_market_attr[id][0] == 1:\n",
    "            train_market_attr[id].pop(0)\n",
    "            train_market_attr[id].insert(0, 0)\n",
    "            train_market_attr[id].insert(1, 1)\n",
    "            train_market_attr[id].insert(2, 0)\n",
    "            train_market_attr[id].insert(3, 0)\n",
    "        elif train_market_attr[id][0] == 2:\n",
    "            train_market_attr[id].pop(0)\n",
    "            train_market_attr[id].insert(0, 0)\n",
    "            train_market_attr[id].insert(1, 0)\n",
    "            train_market_attr[id].insert(2, 1)\n",
    "            train_market_attr[id].insert(3, 0)\n",
    "        elif train_market_attr[id][0] == 3:\n",
    "            train_market_attr[id].pop(0)\n",
    "            train_market_attr[id].insert(0, 0)\n",
    "            train_market_attr[id].insert(1, 0)\n",
    "            train_market_attr[id].insert(2, 0)\n",
    "            train_market_attr[id].insert(3, 1)\n",
    "\n",
    "    for id in test_market_attr:\n",
    "        test_market_attr[id][:] = [x - 1 for x in test_market_attr[id]]\n",
    "        if test_market_attr[id][0] == 0:\n",
    "            test_market_attr[id].pop(0)\n",
    "            test_market_attr[id].insert(0, 1)\n",
    "            test_market_attr[id].insert(1, 0)\n",
    "            test_market_attr[id].insert(2, 0)\n",
    "            test_market_attr[id].insert(3, 0)\n",
    "        elif test_market_attr[id][0] == 1:\n",
    "            test_market_attr[id].pop(0)\n",
    "            test_market_attr[id].insert(0, 0)\n",
    "            test_market_attr[id].insert(1, 1)\n",
    "            test_market_attr[id].insert(2, 0)\n",
    "            test_market_attr[id].insert(3, 0)\n",
    "        elif test_market_attr[id][0] == 2:\n",
    "            test_market_attr[id].pop(0)\n",
    "            test_market_attr[id].insert(0, 0)\n",
    "            test_market_attr[id].insert(1, 0)\n",
    "            test_market_attr[id].insert(2, 1)\n",
    "            test_market_attr[id].insert(3, 0)\n",
    "        elif test_market_attr[id][0] == 3:\n",
    "            test_market_attr[id].pop(0)\n",
    "            test_market_attr[id].insert(0, 0)\n",
    "            test_market_attr[id].insert(1, 0)\n",
    "            test_market_attr[id].insert(2, 0)\n",
    "            test_market_attr[id].insert(3, 1)\n",
    "\n",
    "    label.pop(0)\n",
    "    label.insert(0,'young')\n",
    "    label.insert(1,'teenager')\n",
    "    label.insert(2,'adult')\n",
    "    label.insert(3,'old')\n",
    "    \n",
    "    return train_market_attr, test_market_attr, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_market_attr, test_market_attr, label = import_Market1501Attribute_binary(\"Market-1501_Attribute/\")\n",
    "\n",
    "p = []\n",
    "for pid in train_market_attr.keys():\n",
    "    p.append([pid] + train_market_attr[pid])\n",
    "\n",
    "df = pd.DataFrame(p, columns=['pid'] + label)\n",
    "df.to_csv(\"./Market-1501_Attribute/train_Market_labels.csv\")\n",
    "\n",
    "p = []\n",
    "for pid in test_market_attr.keys():\n",
    "    p.append([pid] + test_market_attr[pid])\n",
    "\n",
    "df = pd.DataFrame(p, columns=['pid'] + label)\n",
    "df.to_csv(\"./Market-1501_Attribute/test_Market_labels.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WIDER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./wider_attribute_annotation/wider_attribute_test.json\", 'r') as f:\n",
    "    test = json.load(f)\n",
    "\n",
    "attribute = test['attribute_id_map']\n",
    "scene = test['scene_id_map']\n",
    "with open(\"./wider_attribute_annotation/wider_attribute_test_attribute_id_map.json\", 'w') as f:\n",
    "    json.dump(attribute, f)\n",
    "with open(\"./wider_attribute_annotation/wider_attribute_test_scene_id_map.json\", 'w') as f:\n",
    "    json.dump(scene, f)\n",
    "\n",
    "p = []\n",
    "for img in test['images']:\n",
    "    sid = img['scene_id']\n",
    "    path = img['file_name']\n",
    "    for target in img['targets']:\n",
    "        att = target['attribute']\n",
    "        bbox = target['bbox']\n",
    "        p.append([path, sid, bbox] + att)\n",
    "\n",
    "attribute_name = []\n",
    "for id in attribute.keys():\n",
    "    attribute_name.append(attribute[id])\n",
    "\n",
    "df = pd.DataFrame(p, columns=['path', 'scene', 'bbox'] + attribute_name)\n",
    "for att_name in attribute_name:\n",
    "    df[att_name].replace(-1, 2, inplace=True)\n",
    "    df[att_name].replace(0, -1, inplace=True)\n",
    "    df[att_name].replace(2, 0, inplace=True)\n",
    "df.to_csv(\"wider_attribute_annotation/wider_attribute_test_annos.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./wider_attribute_annotation/wider_attribute_trainval.json\", 'r') as f:\n",
    "    test = json.load(f)\n",
    "\n",
    "attribute = test['attribute_id_map']\n",
    "scene = test['scene_id_map']\n",
    "with open(\"./wider_attribute_annotation/wider_attribute_trainval_attribute_id_map.json\", 'w') as f:\n",
    "    json.dump(attribute, f)\n",
    "with open(\"./wider_attribute_annotation/wider_attribute_trainval_scene_id_map.json\", 'w') as f:\n",
    "    json.dump(scene, f)\n",
    "\n",
    "p = []\n",
    "for img in test['images']:\n",
    "    sid = img['scene_id']\n",
    "    path = img['file_name']\n",
    "    for target in img['targets']:\n",
    "        att = target['attribute']\n",
    "        bbox = target['bbox']\n",
    "        p.append([path, sid, bbox] + att)\n",
    "\n",
    "attribute_name = []\n",
    "for id in attribute.keys():\n",
    "    attribute_name.append(attribute[id])\n",
    "\n",
    "df = pd.DataFrame(p, columns=['path', 'scene', 'bbox'] + attribute_name)\n",
    "for att_name in attribute_name:\n",
    "    df[att_name].replace(-1, 2, inplace=True)\n",
    "    df[att_name].replace(0, -1, inplace=True)\n",
    "    df[att_name].replace(2, 0, inplace=True)\n",
    "df.to_csv(\"wider_attribute_annotation/wider_attribute_trainval_annos.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
